（爬虫，伪后端，基于文件进行内容交付）
https://github.com/haruki1953/crawl_post_generator

新建分支 
feature-bangumi-list-backend

优化结构之类的二期再说吧，先弄好凑活用吧

## 一期
## 生成数据
新建 bangumi_list.py

获取数据后，调用 bangumi_list 添加数据

调用 bangumi_list 保存数据

根据bash执行时的参数判断是否保存bangumi_list
存在第二参数时：
为'BL'时启用bangumi-list，为'onlyBL'时不保存html

完成，爬一下现有数据


## 二期
- 改用模板引擎
- 优化结构
- 爬取标签、别名
- 优化页面、暗黑模式
- config.json设计 [01-项目文档【config.json设计（二期）】](01-项目文档.md#config.json设计（二期）)


## 改用模板引擎
pip install jinja2
完善一下requirements.txt
```
beautifulsoup4==4.12.2
jinja2==3.1.4
```
pip show jinja2 查看版本
pip install -r requirements.txt 安装

改一下目录
my_crawlers改为crawlers
创建templates文件夹，用于存放html模板
> my_templates暂时保留，最终要删除
> 注意，已经给所有带my的数据去掉了

在templates文件夹下，创建：
- main.html 主模板
- thread-default.html 帖子默认模板
- thread-bangumi.html 番剧模板

[python模板引擎jinja2【案例】](笔记/python模板引擎jinja2.md#案例)
原来可以在模板里进行全部的逻辑判断，这下简单了
generate_html中可以直接用threadDataList调用模板引擎

```
改模板替换正则
\{(\b.*?\b)\}
{{ thread.data.MainPost.$1 }}
```

main.html
```html
<thread-list>
    {% for thread in threadDataList %}
        {% if thread.source == 'default' %}
            {% include 'thread-default.html' %}
        {% elif thread.source == 'bangumi' %}
            {% include 'thread-bangumi.html' %}
        {% endif %}
    {% endfor %}
</thread-list>
```

完成
现在可以删除my_templates

**注意注意：** 最后删除空行是必须的，否则alist的readme可能不会将其解析为html
```python
# 最后删除空行
# **注意注意：** 最后删除空行是必须的，否则alist的readme可能不会将其解析为html
html = re.sub(r'\n\s*\n', '\n', html)
```

## 优化结构
将 bgmDataSavePath 与 alistBasePath 封装至 setting.py

### 改进番剧库兼容性——alistPath改为完整网址
关于bgm_data.json中的 alistPath 字段，二期变为了必须为完整网站路径如：https://bangumi.sakiko.top/Bangumi/GIRLS%20BAND%20CRY
此举可以改进番剧库兼容性，如果别的alist想并入，提供对应的bgm_data.json即可

```python
# alist网站网址，用于与截取的路径拼接
alistWebUrl = 'https://bangumi.sakiko.top'


# 准备 alistPath
alistPath = ''
if 'alistPath' in urlInfo:
	alistPath = urlInfo['alistPath']
else:
	alistPath = alistPathCut(pageInfo['saveFile'])
# 拼接完整网址
alistPath = alistWebUrl + alistPath
```


## 爬取标签、别名
番剧介绍介绍感觉用处不大，不保存了
### 别名
模仿以前写的进行匹配
```python
# 正则表达式匹配获取信息
""" BANGUMI_CHINESE """
match = re.search(
	r'<li><span class="tip">中文名: </span>(.*?)</li>', infoBoxStr)
MainPost['BANGUMI_CHINESE'] = match.group(1) if match else ''

""" BANGUMI_ALIAS_LIST """
matches = re.findall(
	r'<li><span class="tip".*?>别名: </span>(.*?)</li>', infoBoxStr)
MainPost['BANGUMI_ALIAS_LIST'] = matches if matches else []
print(MainPost['BANGUMI_ALIAS_LIST'])
```

## 标签
先找到 div id="subject_detail" 再找到 div class="subject_tag_section" 其中保存着番剧标签，然后用正则表达式匹配
```html
<a class="l" href="/anime/tag/%E7%BE%8E%E9%A3%9F"><span>美食</span>
<a class="l" href="/anime/tag/.*?"><span>(.*?)</span>
```
```python
""" BANGUMI_TAG_LIST """
# 先找到 div id="subject_detail" 再找到 div class="subject_tag_section"
# 其中保存着番剧标签，然后用正则表达式匹配
bangumiTagSection = postSoup.find('div', {'id': 'subject_detail'}).find(
    'div', {'class': 'subject_tag_section'})
tagSectionStr = str(bangumiTagSection)
matches = re.findall(
    r'<a class="l" href="/anime/tag/.*?"><span>(.*?)</span>', tagSectionStr)
MainPost['BANGUMI_TAG_LIST'] = matches if matches else []
print(MainPost['BANGUMI_TAG_LIST'])
```
完成

加入了保存
```
"tagList": data['MainPost']['BANGUMI_TAG_LIST'],
"aliasList": data['MainPost']['BANGUMI_ALIAS_LIST'],
```
测试 python main.py test/crawl_info_老番.json onlyBL
#### 都爬取一下，一会前端要用
改进
```python
# 使用 separators 参数生成紧凑的 JSON 文件 ',', ':' 后将无空格
json.dump(bangumiList, f, ensure_ascii=False, separators=(',', ':'))

# 保存为便于查看的格式 indent=4 指定每一级缩进使用4个空格
json.dump(config, f, ensure_ascii=False, indent=4)
```
python main.py test/crawl_info_老番.json onlyBL
python main.py test/crawl_info_23十月.json onlyBL
python main.py test/crawl_info_24一月.json onlyBL
python main.py test/crawl_info_24四月.json onlyBL

## 页面完善、适配暗黑模式
显示标签，为考虑兼容alist的暗黑模式，将其css变量保存，并按其设计
它是通过 body 上的 class="hope-ui-dark" 来控制暗黑模式的css变量
[alist-var.css](code/alist-var.css)

为测试生成的html加上 `<link rel="stylesheet" href="/test/alist-var.css">` ，即可开始开发
将标签盒子加至 bangumi_content 内的最后，完成
给其他的适配一下alist的暗黑模式，完成

最后改一下模板
python main.py test/crawl_info测试.json



## TODO
- 为项目重写readme



